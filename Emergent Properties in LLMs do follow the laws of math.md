
2025-08-18 18:28

Status:

Tags: [[selfTheory]] [[LLMs(Large language models)]] [[technology]] 




# Emergent Properties in LLMs do follow the laws of math

- In the current world, we are at a race towards who could build the most efficient and powerful Large language models(LLMs)
- In the grand scheme of things, there seems to be an improvement in the abilities of the LLM based on the amount of data it was fed. This seems to be pretty intuitive and reasonable, the more data it can refer and learn from, the better the model gets. Simple.
- The model architecture also do play an important role in the conservative side of the LLM so that it can be more efficient and require less computational power(Thanks to attention mechanisms). 
- With all this said and done, there is still one major misconception that we as humans tend to have these days. Which is that the model suddenly gets new abilities at a certain threshold "magically"
- This article is here to maybe prove that these emergent abilities may not be so "magical" nor "random" after all.
## Why do i believe that these properties are not unusual but actually explainable?

- To answer this question, we might need to step back in the field of math(Especially probability) and take into consideration the theorem proven as the "law of large numbers"
- This may not be very complicated to understand but it does help explain a lot of probability related problems
- To name one such problem is the ability to predict just how much uranium would be needed for an atomic bomb
- Why would that have any relevance here, you may ask? Well that is certainly due to how current world machine learning training works
- In a typical "Chat generative AI" the main goal is to answer a query or input given by the user, right? But one thing that people tend to forget is that the "answer" provided by the chatbot is hugely dependent on the model such as predicting the next word in the sequence it generates or the probability of tokens influencing the attention mechanism of the model
- It is very so often forgotten that current world machine learning is just as similar to creating prediction machines through brute forcing tons of data into the model until it is able to predict better 
- At this point of this article, it would be clear that most of which i said so far makes sense except for the concept of "law of large numbers". The relativity of the law of large numbers to the concept would make sense in the next part of this article
## What exactly does it mean to predict?

- To put it plain and simple, LLMs are predictive machines at it's core. It uses probability as it's core to fully function
- Each part of the LLM from it's neural network structures weights and the prediction of the next token is all a part of probability
- To be precise, this is all a part of "Dependent probability". Why do i claim so? Huge part of the outcome of such training is directly related or influenced by initially the input data and then the following events which are "chained together"
- A model cannot be independent of what forms it's baseline. 
- Most people would at this point say that "If you view it like so, anything can be a dependent probability and anything can be predicted" and you are exactly right.
- If we are capable of knowing which factors affect the current action at hand, while also being able to know how much of an influence each factor has towards the current action at hand then we simply can have a mathematical probability of what could eventually conspire through the current action. This is such that **_If only_** we knew every factor which is affecting the current action at hand. This chain of events which has a certain effect on the current action at hand based on it's flow of sequence is what forms the baseline for what I would call is a "chain of dependent events".
- Each dependent event can possibly predict another dependent event given the fact that it flows in a certain stretch of events. This is true if the dependency isn't 100% and there are other factors affecting the current event. This would give rise to multiple possibilities and the theory of multiple possibilities is the reason of birth for "probability".
- So, what if we assume the opposite and take that the current event is 100% dependent on another event? Then this would cause a simpler flow of events where there is no probability and it is rather a guaranteed result.
- Now, what if we add just a bit of influence from another event to add a bit of variety to the current event so the result is varied? Now, we are back to the previous case where there were multiple probabilities available. But if the amount of influence is known and the influencing events are known. The probability of the result can be calculated. It wouldn't be a guaranteed result but there will be a branch of events where each result branch could be predicted.
- That's great and simple for a small range of dependencies and events. Now what if you add in a bunch of events which can conspire based on the initial dependency and can have a variety of events influencing the current event and the result based on what initially sets off the chain of events? Now you get a range of possibilities and the number of possibilities are limited to the different chain of events which could conspire. This is what initially grew as root for the theory of "Multidimensional universe".
- So, now that we have a base understanding on how predictions work, let's get back to the theory at hand. 
- Since LLMs are models directly dependent on data, there is a good chance for the LLM to not be good at a wide range of tasks since it is narrow-sighted towards the small amount of data it received
- Hence why LLMs are trained using large amounts of data to have the narrow-sight be in a diverse field of areas to the point that it doesn't matter if it is narrow-sighted. This is the same concept of there being no real way of being able to judge the knowledge of a being if it knows everything we know.  Now, why exactly do emergent properties occur? This is explained in the section below

## Relevancy of the law of large numbers to LLMs

- In essence, everything we feed into the LLM is "learned" and "understood" by the LLM through being able to assign the right probability of each token in a certain sequence 
- So, if we feed info on the same type of event over and over again, it eventually reaches a point where the outcome of each individual after that is not so important in the grand scheme of things.
- This is due to there being significantly more previous info than there being any current info fed into the model at a time
- So, if this is true, then the model would reach a sweet spot where it just cant get much better unless there is a large amount of data in more deviation to the previous data being fed into the current model. Which is way less likely considering the context of each token reaching a certain high threshold on what previous token or chain of events could cause it or could only be in a loop of similar patterns due to there not being influential events which are more deviant from the previous events. Since there are not much external influence and the training is recursive, the model behavior is to be set. If the model reaches such a point of being able to predict, then the model would in fact be so overfitted to the possibilities of the token such that there would be no "potential false answers" or hallucinations if there are lesser room of noise and thus lesser room for chain of events beyond the predicted possibilities. "A being cannot be judged to not know enough by the judge if the judge doesn't know more than the being"

## Conclusion

- To remove all confusion, the "emergent properties" are not some kind of "magical appearance in models" but rather just a statistical view on the graph having way less notable growth to the point that the answers become more aligned with the grand scheme of possibilities in the data we fed the model to the point that it hallucinates less of what is not in the data and since the data is mainly just what we as human being mostly are aware of, this level of alignment is directly correlated to us humans as we know no better ourselves and thus make it seem like super intelligence for most.
- To achieve true super intelligence or AGI, we would require a system which is better able to predict what is not given to the model and that is impossible without possible chain of events which can deviate far from the given data. This essentially gave rise to the chain-of-thought models. But essentially, we cannot reach potential AGI nor would we find a way to truly predict all possible events in a wide perspective unless we find a superior architecture where we can conceptually have an architecture to create "potential theories" by adding more noise(added events influencing the current event)(Technically hallucination but in a large scale to form different branches oh hallucination such that one may eventually be true) to the model's predictive capabilities and then being able the predict which of the potential theories have the highest chance of probability and then moving onto exploring that higher probability theory to better make sense and find a new chain of event. Given enough time and computational power, such a system could essentially find every single chain of event in existence and thus in theory could predict anything. That is essentially AGI.




# References